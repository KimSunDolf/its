# スピーチからテキストへ

音声をテキストに変換する方法を学びます。

## 序章

speech-to-text API は、最先端のオープン ソース mega-v2 Whisper モデルに基づいて、文字起こしと翻訳の 2 つのエンドポイントを提供します。それらは次の目的で使用できます。

-   オーディオを任意の言語に書き起こします。
-   オーディオを英語に翻訳して書き起こします。

ファイルのアップロードは現在 25 MB に制限されており、次の入力ファイル タイプがサポートされています: mp3、mp4、mpeg、mpga、m4a、wav、および webm。

## クイックスタート

### 文字起こし

文字起こし API への入力は、文字起こしする音声ファイルと、目的の出力形式の音声文字起こしです。現在、いくつかの入力および出力ファイル形式をサポートしています。

```python
# Note: you need to be using OpenAI Python v0.27.0 for the code below to work
import openai
audio_file= open("/path/to/file/audio.mp3", "rb")
transcript = openai.Audio.transcribe("whisper-1", audio_file)
```

デフォルトでは、応答タイプは未加工のテキストを含む JSON になります。

```text
{
  "text": "Imagine the wildest idea that you've ever had, and you're curious about how it might scale to something that's a 100, a 1,000 times bigger.
....
}
```

リクエストに追加のパラメーターを設定するには、関連するオプションを含む --form 行をさらに追加します。たとえば、出力をテキストとしてフォーマットする場合は、次の行を追加する必要があります。

```
...
--form file=@openai.mp3 \
--form model=whisper-1 \
--form response_format=text
```

### 翻訳

翻訳 API は、サポートされている任意の言語を入力音声ファイルとして受け取り、必要に応じて音声を英語に書き起こします。これは、出力が元の入力言語ではなく、英語のテキストに翻訳されるという点で、/Transcriptions エンドポイントとは異なります。

```
# Note: you need to be using OpenAI Python v0.27.0 for the code below to work
import openai
audio_file= open("/path/to/file/german.mp3", "rb")
transcript = openai.Audio.translate("whisper-1", audio_file)
```

この場合、入力音声はドイツ語で、出力テキストは次のようになります。

```
Hello, my name is Wolfgang and I come from Germany. Where are you heading today?
```

現在、英語の翻訳のみをサポートしています。

## 対応言語

現在、トランスクリプションと翻訳の両方のエンドポイントを通じて、以下の言語をサポートしています。

アフリカーンス語、アラビア語、アルメニア語、アゼルバイジャン語、ベラルーシ語、ボスニア語、ブルガリア語、カタロニア語、中国語、クロアチア語、チェコ語、デンマーク語、オランダ語、英語、エストニア語、フィンランド語、フランス語、ガリシア語、ドイツ語、ギリシャ語、ヘブライ語、ヒンディー語、ハンガリー語、アイスランド語、インドネシア語、イタリア語、

基礎となるモデルは98の言語で訓練されましたが、音声からテキストへのモデルの精度の業界標準ベンチマークである<50%の単語エラー率（WER）を超えた言語のみをリストします。このモデルは、上記に記載されていない言語の結果を返しますが、品質は低くなります。

## もっと長い入力

デフォルトではWhisper APIは25MB未満のファイルのみサポートします。もしこれより長いオーディオファイルがあれば、25MB未満のブロックに分けて、圧縮されたフォーマットを使用する必要があります。最高の性能を得るために、上下の文字情報を失わないように、文章の真ん中で音を切るのを避けてください。この問題に対処する一つの方法はPyDubオープンソースのPythonパッケージを使って音声ファイルを分割することです。

```
from pydub import AudioSegmentsong = AudioSegment.from_mp3("good_morning.mp3")# PyDub handles time in millisecondsten_minutes = 10 * 60 * 1000first_10_minutes = song[:ten_minutes]first_10_minutes.export("good_morning_10.mp3", format="mp3")
```

OpenAIはPyDubのような第三者ソフトウェアの有用性や安全性についていかなる保証もしません。

## ヒント

ヒントを使用して、Whisper API によって生成される文字起こしの品質を向上させることができます。モデルはプロンプトのスタイルに合わせようとするため、プロンプトで大文字と句読点も使用されている場合は、それらが使用される可能性が高くなります。ただし、現在のヒント システムは、他の言語モデルよりもはるかに制限されており、生成された音声に対する制御は限られています。さまざまな状況でヒントを使用する方法の例を次に示します。

1.  モデルが音声内の特定の単語または略語を頻繁に誤認する場合に非常に役立ちます。たとえば、次のヒントは、単語 DALL·E および GPT-3 (以前は「GDP 3」および「DALI」と書かれていた) の書き起こしを改善します。

```
この転写はOpenAIがDALL・E、GPT-3、ChatGPTなどの技術を開発し、いつか人類すべての人に利益をもたらすAGIシステムを構築することを望んでいます。
```

2.  セグメント化されたファイルのコンテキストを保持するには、以前のセグメントの文字起こしを使用してモデルをブートストラップします。これにより、モデルが以前の音声からの関連情報を利用するため、文字起こしがより正確になります。モデルは最後の 224 トークンのみを考慮し、それより前のものは無視します。
3.  場合によっては、句読点が転記でスキップされることがあります。これは、句読点を含む簡単なヒントを使用することで回避できます。

2.  モデルは、音声で一般的なフィラー ワードを省略する場合もあります。書き起こしにつなぎ言葉を残したい場合は、ディレクティブを使用してそれらを含めることができます。

```
 うん...ちょっと考えさせてください、ええと...よし、これが私が考えていることだ。
```

3.  簡体字中国語や繁体字中国語など、一部の言語は別の方法で記述できます。デフォルトでは、モデルは常に目的の文体を処理できるとは限りません。この問題は、好みの文体に関する指示を追加することで改善できます。